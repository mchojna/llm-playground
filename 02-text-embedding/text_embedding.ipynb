{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855cd1fd",
   "metadata": {},
   "source": [
    "# Text Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327b243c",
   "metadata": {},
   "source": [
    "Resource: https://www.deeplearning.ai/short-courses/google-cloud-vertex-ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12910b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacce61f",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a033fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import base64\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.service_account import Credentials\n",
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "import numpy as np\n",
    "\n",
    "def authenticate():\n",
    "    return \"DALI-credentials\", \"DLAI-PROJECT\"\n",
    "    #Load .env\n",
    "    load_dotenv()\n",
    "    \n",
    "    #Decode key and store in .JSON\n",
    "    SERVICE_ACCOUNT_KEY_STRING_B64 = os.getenv('SERVICE_ACCOUNT_KEY')\n",
    "    SERVICE_ACCOUNT_KEY_BYTES_B64 = SERVICE_ACCOUNT_KEY_STRING_B64.encode(\"ascii\")\n",
    "    SERVICE_ACCOUNT_KEY_STRING_BYTES = base64.b64decode(SERVICE_ACCOUNT_KEY_BYTES_B64)\n",
    "    SERVICE_ACCOUNT_KEY_STRING = SERVICE_ACCOUNT_KEY_STRING_BYTES.decode(\"ascii\")\n",
    "\n",
    "    SERVICE_ACCOUNT_KEY = json.loads(SERVICE_ACCOUNT_KEY_STRING)\n",
    "\n",
    "\n",
    "    # Create credentials based on key from service account\n",
    "    # Make sure your account has the roles listed in the Google Cloud Setup section\n",
    "    credentials = Credentials.from_service_account_info(\n",
    "        SERVICE_ACCOUNT_KEY,\n",
    "        scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "\n",
    "    if credentials.expired:\n",
    "        credentials.refresh(Request())\n",
    "    \n",
    "    #Set project ID accoridng to environment variable    \n",
    "    PROJECT_ID = os.getenv('PROJECT_ID')\n",
    "        \n",
    "    return credentials, PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a254ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(data, x_labels=None, y_labels=None, title=None):\n",
    "    fig, ax = plt.subplots(figsize=(50, 3))\n",
    "    heatmap = ax.pcolor(data, cmap='coolwarm', edgecolors='k', linewidths=0.1)\n",
    "\n",
    "    # Add color bar to the right of the heatmap\n",
    "    cbar = plt.colorbar(heatmap, ax=ax)\n",
    "    cbar.remove()\n",
    "\n",
    "    # Set labels for each axis\n",
    "    if x_labels:\n",
    "        ax.set_xticks(np.arange(data.shape[1]) + 0.5, minor=False)\n",
    "        ax.set_xticklabels(x_labels, rotation=45, ha=\"right\")\n",
    "    if y_labels:\n",
    "        ax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)\n",
    "        ax.set_yticklabels(y_labels, va=\"center\")\n",
    "\n",
    "    # Set title\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "def plot_2D(x_values, y_values, labels):\n",
    "\n",
    "    # Create scatter plot\n",
    "    fig, ax = plt.subplots()\n",
    "    scatter = ax.scatter(x_values, \n",
    "                         y_values, \n",
    "                         alpha = 0.5, \n",
    "                         edgecolors='k',\n",
    "                         s = 40) \n",
    "\n",
    "    # Create a mplcursors object to manage the data point interaction\n",
    "    cursor = mplcursors.cursor(scatter, hover=True)\n",
    "\n",
    "    #aes\n",
    "    ax.set_title('Embedding visualization in 2D')  # Add a title\n",
    "    ax.set_xlabel('X_1')  # Add x-axis label\n",
    "    ax.set_ylabel('X_2')  # Add y-axis label\n",
    "\n",
    "    # Define how each annotation should look\n",
    "    @cursor.connect(\"add\")\n",
    "    def on_add(sel):\n",
    "        sel.annotation.set_text(labels[sel.target.index])\n",
    "        sel.annotation.get_bbox_patch().set(facecolor='white', alpha=0.5) # Set annotation's background color\n",
    "        sel.annotation.set_fontsize(12) \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(sentences, batch_size = 5):\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i : i + batch_size]\n",
    "\n",
    "def encode_texts_to_embeddings(sentences):\n",
    "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception:\n",
    "        return [None for _ in range(len(sentences))]\n",
    "        \n",
    "def encode_text_to_embedding_batched(sentences, api_calls_per_second = 0.33, batch_size = 5):\n",
    "    # Generates batches and calls embedding API\n",
    "    \n",
    "    embeddings_list = []\n",
    "\n",
    "    # Prepare the batches using a generator\n",
    "    batches = generate_batches(sentences, batch_size)\n",
    "\n",
    "    seconds_per_job = 1 / api_calls_per_second\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch in tqdm(\n",
    "            batches, total = math.ceil(len(sentences) / batch_size), position=0\n",
    "        ):\n",
    "            futures.append(\n",
    "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
    "            )\n",
    "            time.sleep(seconds_per_job)\n",
    "\n",
    "        for future in futures:\n",
    "            embeddings_list.extend(future.result())\n",
    "\n",
    "    is_successful = [\n",
    "        embedding is not None for sentence, embedding in zip(sentences, embeddings_list)\n",
    "    ]\n",
    "    embeddings_list_successful = np.squeeze(\n",
    "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
    "    )\n",
    "    return embeddings_list_successful\n",
    "\n",
    "def clusters_2D(x_values, y_values, labels, kmeans_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    scatter = ax.scatter(x_values, \n",
    "                         y_values, \n",
    "                         c = kmeans_labels, \n",
    "                         cmap='Set1', \n",
    "                         alpha=0.5, \n",
    "                         edgecolors='k', \n",
    "                         s = 40)  # Change the denominator as per n_clusters\n",
    "\n",
    "    # Create a mplcursors object to manage the data point interaction\n",
    "    cursor = mplcursors.cursor(scatter, hover=True)\n",
    "\n",
    "    #axes\n",
    "    ax.set_title('Embedding clusters visualization in 2D')  # Add a title\n",
    "    ax.set_xlabel('X_1')  # Add x-axis label\n",
    "    ax.set_ylabel('X_2')  # Add y-axis label\n",
    "\n",
    "    # Define how each annotation should look\n",
    "    @cursor.connect(\"add\")\n",
    "    def on_add(sel):\n",
    "        sel.annotation.set_text(labels.category[sel.target.index])\n",
    "        sel.annotation.get_bbox_patch().set(facecolor='white', alpha=0.95) # Set annotation's background color\n",
    "        sel.annotation.set_fontsize(14) \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa01689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(sentences, batch_size = 5):\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i : i + batch_size]\n",
    "\n",
    "def encode_texts_to_embeddings(sentences):\n",
    "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception:\n",
    "        return [None for _ in range(len(sentences))]\n",
    "        \n",
    "def encode_text_to_embedding_batched(sentences, api_calls_per_second = 0.33, batch_size = 5):\n",
    "    # Generates batches and calls embedding API\n",
    "    \n",
    "    embeddings_list = []\n",
    "\n",
    "    # Prepare the batches using a generator\n",
    "    batches = generate_batches(sentences, batch_size)\n",
    "\n",
    "    seconds_per_job = 1 / api_calls_per_second\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch in tqdm(\n",
    "            batches, total = math.ceil(len(sentences) / batch_size), position=0\n",
    "        ):\n",
    "            futures.append(\n",
    "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
    "            )\n",
    "            time.sleep(seconds_per_job)\n",
    "\n",
    "        for future in futures:\n",
    "            embeddings_list.extend(future.result())\n",
    "\n",
    "    is_successful = [\n",
    "        embedding is not None for sentence, embedding in zip(sentences, embeddings_list)\n",
    "    ]\n",
    "    embeddings_list_successful = np.squeeze(\n",
    "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
    "    )\n",
    "    return embeddings_list_successful\n",
    "\n",
    "# configure ScaNN as a tree - asymmetric hash hybrid with reordering\n",
    "# anisotropic quantization as described in the paper; see README\n",
    "def create_index(embedded_dataset, \n",
    "                 num_leaves,\n",
    "                 num_leaves_to_search,\n",
    "                 training_sample_size):\n",
    "    \n",
    "    # normalize data to use cosine sim as explained in the paper\n",
    "    normalized_dataset = embedded_dataset / np.linalg.norm(embedded_dataset, axis=1)[:, np.newaxis]\n",
    "    \n",
    "    searcher = (\n",
    "        scann.scann_ops_pybind.builder(normalized_dataset, 10, \"dot_product\")\n",
    "        .tree(\n",
    "            num_leaves = num_leaves,\n",
    "            num_leaves_to_search = num_leaves_to_search,\n",
    "            training_sample_size = training_sample_size,\n",
    "        )\n",
    "        .score_ah(2, anisotropic_quantization_threshold = 0.2)\n",
    "        .reorder(100)\n",
    "        .build()\n",
    "    )\n",
    "    return searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03504aae",
   "metadata": {},
   "source": [
    "## Embbedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f29bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c82a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize the Vertex AI Python SDK\n",
    "\n",
    "import vertexai\n",
    "vertexai.init(project = PROJECT_ID, \n",
    "              location = REGION, \n",
    "              credentials = credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e488ab",
   "metadata": {},
   "source": [
    "### Try embbedings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049eeff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d6699",
   "metadata": {},
   "source": [
    "### Text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff785ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = TextEmbeddingModel.from_pretrained(\n",
    "    \"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f54a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embedding_model.get_embeddings(\n",
    "    [\"life\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embedding[0].values\n",
    "print(f\"Length = {len(vector)}\")\n",
    "print(vector[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embedding_model.get_embeddings(\n",
    "    [\"What is the meaning of life?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1842f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embedding[0].values\n",
    "print(f\"Length = {len(vector)}\")\n",
    "print(vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e048a76",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a237e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_1 = embedding_model.get_embeddings(\n",
    "    [\"What is the meaning of life?\"]) # 42!\n",
    "\n",
    "emb_2 = embedding_model.get_embeddings(\n",
    "    [\"How does one spend their time well on Earth?\"])\n",
    "\n",
    "emb_3 = embedding_model.get_embeddings(\n",
    "    [\"Would you like a salad?\"])\n",
    "\n",
    "vec_1 = [emb_1[0].values]\n",
    "vec_2 = [emb_2[0].values]\n",
    "vec_3 = [emb_3[0].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_similarity(vec_1,vec_2)) \n",
    "print(cosine_similarity(vec_2,vec_3))\n",
    "print(cosine_similarity(vec_1,vec_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4754481f",
   "metadata": {},
   "source": [
    "### Sentence embbedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82843f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_1 = \"The kids play in the park.\"\n",
    "in_2 = \"The play was for kids in the park.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5484d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_pp_1 = [\"kids\", \"play\", \"park\"]\n",
    "in_pp_2 = [\"play\", \"kids\", \"park\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7938a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_1 = [emb.values for emb in embedding_model.get_embeddings(in_pp_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "emb_array_1 = np.stack(embeddings_1)\n",
    "print(emb_array_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e70b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2 = [emb.values for emb in embedding_model.get_embeddings(in_pp_2)]\n",
    "emb_array_2 = np.stack(embeddings_2)\n",
    "print(emb_array_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe175ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_1_mean = emb_array_1.mean(axis = 0) \n",
    "print(emb_1_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_2_mean = emb_array_2.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emb_1_mean[:4])\n",
    "print(emb_2_mean[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_1)\n",
    "print(in_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_1 = embedding_model.get_embeddings([in_1])\n",
    "embedding_2 = embedding_model.get_embeddings([in_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_1 = embedding_1[0].values\n",
    "print(vector_1[:4])\n",
    "vector_2 = embedding_2[0].values\n",
    "print(vector_2[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a50f3e",
   "metadata": {},
   "source": [
    "## Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize the Vertex AI Python SDK\n",
    "\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, \n",
    "              location=REGION, \n",
    "              credentials = credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45f33bc",
   "metadata": {},
   "source": [
    "### Meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_1 = \"Missing flamingo discovered at swimming pool\"\n",
    "\n",
    "in_2 = \"Sea otter spotted on surfboard by beach\"\n",
    "\n",
    "in_3 = \"Baby panda enjoys boat ride\"\n",
    "\n",
    "\n",
    "in_4 = \"Breakfast themed food truck beloved by all!\"\n",
    "\n",
    "in_5 = \"New curry restaurant aims to please!\"\n",
    "\n",
    "\n",
    "in_6 = \"Python developers are wonderful people\"\n",
    "\n",
    "in_7 = \"TypeScript, C++ or Java? All are great!\" \n",
    "\n",
    "\n",
    "input_text_lst_news = [in_1, in_2, in_3, in_4, in_5, in_6, in_7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "\n",
    "embedding_model = TextEmbeddingModel.from_pretrained(\n",
    "    \"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for input_text in input_text_lst_news:\n",
    "    emb = embedding_model.get_embeddings(\n",
    "        [input_text])[0].values\n",
    "    embeddings.append(emb)\n",
    "    \n",
    "embeddings_array = np.array(embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ce950",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \" + str(embeddings_array.shape))\n",
    "print(embeddings_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac4b6b",
   "metadata": {},
   "source": [
    "### Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA for 2D visualization\n",
    "PCA_model = PCA(n_components = 2)\n",
    "PCA_model.fit(embeddings_array)\n",
    "new_values = PCA_model.transform(embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \" + str(new_values.shape))\n",
    "print(new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "%matplotlib ipympl\n",
    "\n",
    "from utils import plot_2D\n",
    "plot_2D(new_values[:,0], new_values[:,1], input_text_lst_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535fed7a",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_1 = \"\"\"He couldnâ€™t desert \n",
    "          his post at the power plant.\"\"\"\n",
    "\n",
    "in_2 = \"\"\"The power plant needed \n",
    "          him at the time.\"\"\"\n",
    "\n",
    "in_3 = \"\"\"Cacti are able to \n",
    "          withstand dry environments.\"\"\" \n",
    "\n",
    "in_4 = \"\"\"Desert plants can \n",
    "          survive droughts.\"\"\" \n",
    "\n",
    "input_text_lst_sim = [in_1, in_2, in_3, in_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10271b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for input_text in input_text_lst_sim:\n",
    "    emb = embedding_model.get_embeddings([input_text])[0].values\n",
    "    embeddings.append(emb)\n",
    "    \n",
    "embeddings_array = np.array(embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = input_text_lst_sim\n",
    "\n",
    "# Plot the heatmap\n",
    "plot_heatmap(embeddings_array, y_labels = y_labels, title = \"Embeddings Heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec39379",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236387de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(embeddings,idx1,idx2):\n",
    "    return cosine_similarity([embeddings[idx1]],[embeddings[idx2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9992a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_1)\n",
    "print(in_2)\n",
    "print(compare(embeddings,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1952a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_1)\n",
    "print(in_4)\n",
    "print(compare(embeddings,0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635821b8",
   "metadata": {},
   "source": [
    "## Application of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ed9a9",
   "metadata": {},
   "source": [
    "### Load stackoverflow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631aaefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bq_query(sql):\n",
    "\n",
    "    # Create BQ client\n",
    "    bq_client = bigquery.Client(project = PROJECT_ID, \n",
    "                                credentials = credentials)\n",
    "\n",
    "    # Try dry run before executing query to catch any errors\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True, \n",
    "                                         use_query_cache=False)\n",
    "    bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    # If dry run succeeds without errors, proceed to run query\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client_result = bq_client.query(sql, \n",
    "                                    job_config=job_config)\n",
    "\n",
    "    job_id = client_result.job_id\n",
    "\n",
    "    # Wait for query/job to finish running. then get & return data frame\n",
    "    df = client_result.result().to_arrow().to_pandas()\n",
    "    print(f\"Finished job_id: {job_id}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of programming language tags we want to query\n",
    "\n",
    "language_list = [\"python\", \"html\", \"r\", \"css\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b8e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_df = pd.DataFrame()\n",
    "\n",
    "for language in language_list:\n",
    "    \n",
    "    print(f\"generating {language} dataframe\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        CONCAT(q.title, q.body) as input_text,\n",
    "        a.body AS output_text\n",
    "    FROM\n",
    "        `bigquery-public-data.stackoverflow.posts_questions` q\n",
    "    JOIN\n",
    "        `bigquery-public-data.stackoverflow.posts_answers` a\n",
    "    ON\n",
    "        q.accepted_answer_id = a.id\n",
    "    WHERE \n",
    "        q.accepted_answer_id IS NOT NULL AND \n",
    "        REGEXP_CONTAINS(q.tags, \"{language}\") AND\n",
    "        a.creation_date >= \"2020-01-01\"\n",
    "    LIMIT \n",
    "        500\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    language_df = run_bq_query(query)\n",
    "    language_df[\"category\"] = language\n",
    "    so_df = pd.concat([so_df, language_df], \n",
    "                      ignore_index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7572013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you get any errors or you don't want to wait for the query to be completed\n",
    "# so_df = pd.read_csv('so_database_app.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5a214",
   "metadata": {},
   "source": [
    "### Generate text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe27ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextEmbeddingModel.from_pretrained(\n",
    "    \"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebcf422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf2bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function to yield batches of sentences\n",
    "\n",
    "def generate_batches(sentences, batch_size = 5):\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f07fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_questions = so_df[0:200].input_text.tolist() \n",
    "batches = generate_batches(sentences = so_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ee811",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(batches)\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12f4090",
   "metadata": {},
   "source": [
    "### Get embeddings on a batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e238666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts_to_embeddings(sentences):\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception:\n",
    "        return [None for _ in range(len(sentences))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_embeddings = encode_texts_to_embeddings(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d85a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{len(batch_embeddings)} embeddings of size \\\n",
    "{len(batch_embeddings[0])}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c9f32c",
   "metadata": {},
   "source": [
    "### Get entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb1233",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_questions = so_df.input_text.tolist()\n",
    "question_embeddings = encode_text_to_embedding_batched(\n",
    "                            sentences=so_questions,\n",
    "                            api_calls_per_second = 20/60, \n",
    "                            batch_size = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25baee8",
   "metadata": {},
   "source": [
    "### Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbaf6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_df = pd.read_csv(\"../data/so_database_app.csv\")\n",
    "so_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515fd54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/question_embeddings_app.pkl\", 'rb') as file:\n",
    "    question_embeddings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4cb2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \" + str(question_embeddings.shape))\n",
    "print(question_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87d688",
   "metadata": {},
   "source": [
    "### Cluster embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1133df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_dataset = question_embeddings[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e421c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "kmeans = KMeans(n_clusters=n_clusters, \n",
    "                random_state=0, \n",
    "                n_init = 'auto').fit(clustering_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f5036",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_model = PCA(n_components=2)\n",
    "PCA_model.fit(clustering_dataset)\n",
    "new_values = PCA_model.transform(clustering_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e937e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12efadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_2D(x_values = new_values[:,0], y_values = new_values[:,1], \n",
    "            labels = so_df[:1000], kmeans_labels = kmeans_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f78e06",
   "metadata": {},
   "source": [
    "### Anomaly / outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"I am making cookies but don't \n",
    "                remember the correct ingredient proportions. \n",
    "                I have been unable to find \n",
    "                anything on the web.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.get_embeddings([input_text])[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03221fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_l = question_embeddings.tolist()\n",
    "embeddings_l.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff76d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = np.array(embeddings_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbc10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \" + str(embeddings_array.shape))\n",
    "print(embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the outlier text to the end of the stack overflow dataframe\n",
    "so_df = pd.read_csv('so_database_app.csv')\n",
    "new_row = pd.Series([input_text, None, \"baking\"], \n",
    "                    index=so_df.columns)\n",
    "so_df.loc[len(so_df)+1] = new_row\n",
    "so_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce17830",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = IsolationForest(contamination=0.005, \n",
    "                      random_state = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c6542",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.fit_predict(embeddings_array)\n",
    "\n",
    "print(f\"{len(preds)} predictions. Set of possible values: {set(preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_df.loc[preds == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cedc962",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_df = so_df.drop(so_df.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d8fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b1432d",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fed155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec51b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-load the dataset from file\n",
    "so_df = pd.read_csv(\"../data/so_database_app.csv\")\n",
    "X = question_embeddings\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414733f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = so_df['category'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea2880",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0211070",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68a2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred) # compute accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee01b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number between 0 and 1999\n",
    "i = 2\n",
    "label = so_df.loc[i,'category']\n",
    "question = so_df.loc[i,'input_text']\n",
    "\n",
    "# get the embedding of this question and predict its category\n",
    "question_embedding = model.get_embeddings([question])[0].values\n",
    "pred = clf.predict([question_embedding])\n",
    "\n",
    "print(f\"For question {i}, the prediction is `{pred[0]}`\")\n",
    "print(f\"The actual label is `{label}`\")\n",
    "print(\"The question text is:\")\n",
    "print(\"-\"*50)\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f552a1",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1fd38a",
   "metadata": {},
   "source": [
    "### Prompt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, \n",
    "              location=REGION, \n",
    "              credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63750a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\n",
    "    \"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca0f99",
   "metadata": {},
   "source": [
    "### Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac8b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I'm a high school student. \\\n",
    "Recommend me a programming activity to improve my skills.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eaa6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generation_model.predict(prompt=prompt).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89af96d",
   "metadata": {},
   "source": [
    "### Classify and elaborate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"I'm a high school student. \\\n",
    "Which of these activities do you suggest and why:\n",
    "a) learn Python\n",
    "b) learn Javascript\n",
    "c) learn Fortran\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e62d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generation_model.predict(prompt=prompt).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f85bba",
   "metadata": {},
   "source": [
    "### Extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c9f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" A bright and promising wildlife biologist \\\n",
    "named Jesse Plank (Amara Patel) is determined to make her \\\n",
    "mark on the world. \n",
    "Jesse moves to Texas for what she believes is her dream job, \n",
    "only to discover a dark secret that will make \\\n",
    "her question everything. \n",
    "In the new lab she quickly befriends the outgoing \\\n",
    "lab tech named Maya Jones (Chloe Nguyen), \n",
    "and the lab director Sam Porter (Fredrik Johansson). \n",
    "Together the trio work long hours on their research \\\n",
    "in a hope to change the world for good. \n",
    "Along the way they meet the comical \\\n",
    "Brenna Ode (Eleanor Garcia) who is a marketing lead \\\n",
    "at the research institute, \n",
    "and marine biologist Siri Teller (Freya Johansson).\n",
    "\n",
    "Extract the characters, their jobs \\\n",
    "and the actors who played them from the above message as a table\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generation_model.predict(prompt=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1ea00",
   "metadata": {},
   "source": [
    "### Adjust randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2467bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Complete the sentence: \\\n",
    "As I prepared the picture frame, \\\n",
    "I reached into my toolkit to fetch my:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generation_model.predict(\n",
    "    prompt=prompt,\n",
    "    temperature=temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea74cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[temperature = {temperature}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b08369",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generation_model.predict(\n",
    "    prompt=prompt,\n",
    "    temperature=temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[temperature = {temperature}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a3989",
   "metadata": {},
   "source": [
    "### Top P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e880b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab94ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write an advertisement for jackets \\\n",
    "that involves blue elephants and avocados.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec78341",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generation_model.predict(\n",
    "    prompt=prompt, \n",
    "    temperature=0.9, \n",
    "    top_p=top_p,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178d5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[top_p = {top_p}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792709b",
   "metadata": {},
   "source": [
    "### Top K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0873d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "top_p = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generation_model.predict(\n",
    "    prompt=prompt, \n",
    "    temperature=0.9, \n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f53f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[top_p = {top_p}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464007d1",
   "metadata": {},
   "source": [
    "## Semantic Search, Building a Q&A System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32037131",
   "metadata": {},
   "source": [
    "### Load stackoverflow questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a755b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_database = pd.read_csv(\"../data/so_database_app.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47946bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \" + str(so_database.shape))\n",
    "print(so_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ebe402",
   "metadata": {},
   "source": [
    "### Load question embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d39b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e48d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = TextEmbeddingModel.from_pretrained(\n",
    "    \"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5444ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_questions = so_database.input_text.tolist()\n",
    "question_embeddings = encode_text_to_embedding_batched(\n",
    "            sentences = so_questions,\n",
    "            api_calls_per_second = 20/60, \n",
    "            batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52460fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../data/question_embeddings_app.pkl\", \"rb\") as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    question_embeddings = pickle.load(file)\n",
    "  \n",
    "    print(question_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03476780",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_database['embeddings'] = question_embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31abd7af",
   "metadata": {},
   "source": [
    "### Semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances_argmin as distances_argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ea692",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['How to concat dataframes pandas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embedding_model.get_embeddings(query)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f306f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_array = cosine_similarity([query_embedding],\n",
    "                                  list(so_database.embeddings.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6650dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_doc_cosine = np.argmax(cos_sim_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee80d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_doc_distances = distances_argmin([query_embedding], \n",
    "                                       list(so_database.embeddings.values))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe28cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_database.input_text[index_doc_cosine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_database.output_text[index_doc_cosine]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca87465e",
   "metadata": {},
   "source": [
    "### Question answering with relevant context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b0c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fdfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\n",
    "    \"text-bison@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11fd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Question: \" + so_database.input_text[index_doc_cosine] +\\\n",
    "\"\\n Answer: \" + so_database.output_text[index_doc_cosine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa58c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Here is the context: {context}\n",
    "             Using the relevant information from the context,\n",
    "             provide an answer to the query: {query}.\"\n",
    "             If the context doesn't provide \\\n",
    "             any relevant information, \\\n",
    "             answer with \\\n",
    "             [I couldn't find a good match in the \\\n",
    "             document database for your query]\n",
    "             \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "t_value = 0.2\n",
    "response = generation_model.predict(prompt = prompt,\n",
    "                                    temperature = t_value,\n",
    "                                    max_output_tokens = 1024)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d5e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['How to make the perfect lasagna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embedding_model.get_embeddings(query)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ede03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_array = cosine_similarity([query_embedding], \n",
    "                                  list(so_database.embeddings.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6408dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_doc = np.argmax(cos_sim_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952ffd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = so_database.input_text[index_doc] + \\\n",
    "\"\\n Answer: \" + so_database.output_text[index_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Here is the context: {context}\n",
    "             Using the relevant information from the context,\n",
    "             provide an answer to the query: {query}.\"\n",
    "             If the context doesn't provide \\\n",
    "             any relevant information, answer with \n",
    "             [I couldn't find a good match in the \\\n",
    "             document database for your query]\n",
    "             \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4166c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_value = 0.2\n",
    "response = generation_model.predict(prompt = prompt,\n",
    "                                    temperature = t_value,\n",
    "                                    max_output_tokens = 1024)\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95feea6a",
   "metadata": {},
   "source": [
    "### Scale with approximate nearest neighbor search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scann\n",
    "\n",
    "#Create index using scann\n",
    "index = create_index(embedded_dataset = question_embeddings, \n",
    "                     num_leaves = 25,\n",
    "                     num_leaves_to_search = 10,\n",
    "                     training_sample_size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how to concat dataframes pandas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "start = time.time()\n",
    "query_embedding = embedding_model.get_embeddings([query])[0].values\n",
    "neighbors, distances = index.search(query_embedding, final_num_neighbors = 1)\n",
    "end = time.time()\n",
    "\n",
    "for id, dist in zip(neighbors, distances):\n",
    "    print(f\"[docid:{id}] [{dist}] -- {so_database.input_text[int(id)][:125]}...\")\n",
    "\n",
    "print(\"Latency (ms):\", 1000 * (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "query_embedding = embedding_model.get_embeddings([query])[0].values\n",
    "cos_sim_array = cosine_similarity([query_embedding], list(so_database.embeddings.values))\n",
    "index_doc = np.argmax(cos_sim_array)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"[docid:{index_doc}] [{np.max(cos_sim_array)}] -- {so_database.input_text[int(index_doc)][:125]}...\")\n",
    "\n",
    "print(\"Latency (ms):\", 1000 * (end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
