{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc101e5",
   "metadata": {},
   "source": [
    "# Fine tuning LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac90e4e",
   "metadata": {},
   "source": [
    "Resource: https://www.deeplearning.ai/short-courses/finetuning-large-language-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd4455",
   "metadata": {},
   "source": [
    "## Compare finetuned and non-finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lamini\n",
    "\n",
    "lamini.api_url = os.getenv(\"POWERML_PRODUCTION_URL\")\n",
    "lamini.api_key = os.getenv(\"POWERML_PRODUCTION_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModelRunner:\n",
    "    def __init__(self, model_name):\n",
    "        if model_name == \"meta-llama/Llama-2-7b-hf\":\n",
    "            self.responses = {\n",
    "                \"Tell me how to train my dog to sit\": \"Tell me how to train my dog to stay.\\nTell me how to teach my dog to come.\\nTell me how to get my dog to heel.\\nTell me how to stop my dog from jumping.\\nTell me how to make my dog stop barking.\\nTell me how I can get my dog to stop chewing.\\nTell me how to house train my dog.\\nTell me how to potty train my dog.\\nTell me how to train my dog to walk on a leash.\\nTell me how to crate train my dog.\\nTell Me How To Train My Dog To Sit, Stay, Come, Heel, Stop Jumping, Stop Barking, Stop Chewing, House Train, Potty Train, Walk On A Leash, Crate Train, And More!\\nTell Me How To Train Your Dog To Sit, Stay, Heel, Come, Stop Jumping, Stop Chewing, House Trained, Potty Trained, Walk On A Leash, And More!\\nTell me how to train your dog to sit.\\nTell Me How To House Train My Dog.\",\n",
    "                \"[INST]Tell me how to train my dog to sit[/INST]\": \"[INST]Tell me how to train my dog to sit[/INST]\",\n",
    "                \"What do you think of Mars?\": \"I think it's a great planet.\\nI think it's a good planet.\\nI think it'll be a great planet.\\nI think we should go there.\\nI think we should go back there.\\nI think we should stay there.\\nI think we should leave there.\\nI think we should colonize there.\\nI think we should terraform there.\\nI think we should mine there.\\nI think we should build there.\\nI think we should live there.\\nI think we should die there.\\nI think we should be there.\\nI think we should have been there.\\nI think we should never be there.\\nI think we'll be there.\\nI think we won't be there.\\nI think we will be there.\\nI think we can be there.\\nI think we could be there.\\nI think we would be there.\\nI think we might be there.\\nI think we may be there.\\nI think we shall be there.\\nI think we must be there.\\nI think we have to be there.\\nI think we need to be there.\\nI think I'll be there.\\nI know I'll be there.\",\n",
    "                \"taylor swift's best friend\": \"I'm not sure if I've mentioned this before, but I'm a huge Taylor Swift fan.\\nI've been a fan since her first album, and I've been a fan ever since. I've been a fan of her music, her style, her personality, and her music. I've been a fan for a long time.\\nI've been a fan of Taylor Swift for a long time. I've been a fan of her music for a long time. I'm a fan of her style, her personality, her music, and her music. I've been a fan for a long, long time.\\nI've always been a fan of Taylor Swift. I've always been a fan of her music.\\nI've always been a big fan of her style. I've always been a huge fan of her personality. I've always been a massive fan of her music. I'm a fan of Taylor Swift. I love her music. I love her style. I love her personality. I love her music. I'm a huge fan of Taylor Swift.\\nI've always been an avid fan of Taylor Swift. I'm\",\n",
    "                \"Agent: I'm here to help you with your Amazon deliver order.\\nCustomer: I didn't get my item\\nAgent: I'm sorry to hear that. Which item was it?\\nCustomer: the blanket\\nAgent:\": \"I'm sorry to hear that but I'm here to help.\\nCustomer: I don't get the blanket\\nCustomer: I don't have the blanket\\nAgent: I don't understand.\\nAgent: I'm sorry. I don't understand. customer: I don't have the blanket Agent: I'm so sorry to hear that.\\nCustomer: I don t have the blanket\\nAgent: Oh no, I'm so sorry to hear about that.\\ncustomer: I\",\n",
    "            }\n",
    "        elif model_name == \"meta-llama/Llama-2-7b-chat-hf\":\n",
    "            self.responses = {\n",
    "                \"Tell me how to train my dog to sit\": \"on command. Training your dog to sit is a basic obedience command that can be achieved with patience, consistency, and positive reinforcement. Here's a step-by-step guide on how to train your dog to sit:\\n\\n1. Choose a quiet and distraction-free area: Find a quiet room or area where your dog can focus on you without getting distracted.\\n2. Have treats ready: Choose your dog's favorite treats and have them readily available to reward good behavior.\\n3. Stand in front of your dog: Stand in front of your dog and hold a treat close to their nose.\\n4. Move the treat up and back: Slowly move the treat upwards and backwards, towards your dog's tail, while saying \\\"sit\\\" in a calm and clear voice.\\n5. Dog will follow the treat: As you move the treat, your dog should naturally sit down to follow it. The moment they touch their bottom to the ground, give them the treat and praise them.\",\n",
    "                \"[INST]Tell me how to train my dog to sit[/INST]\": 'Training your dog to sit is a basic obedience command that can be achieved with patience, consistency, and positive reinforcement. Here\\'s a step-by-step guide on how to train your dog to sit:\\n\\n1. Choose a quiet and distraction-free area: Find a quiet room or area where your dog can focus on you without getting distracted.\\n2. Have treats ready: Choose your dog\\'s favorite treats and have them readily available to reward good behavior.\\n3. Stand in front of your dog: Stand in front of your dog and hold a treat close to their nose.\\n4. Move the treat up and back: Slowly move the treat upwards and backwards, towards your dog\\'s tail, while saying \"sit\" in a calm and clear voice.\\n5. Dog will follow the treat: As you move the treat, your dog should naturally sit down to follow it. The moment they touch their bottom to the ground, give them the treat and praise them.\\n6. Repeat the process: Repeat steps 3-5 several times, so your dog learns to associate the command \"sit\" with the action of sitting down.',\n",
    "                \"What do you think of Mars?\": \"Mars is a fascinating planet that has captured the imagination of humans for centuries. It is the fourth planet from the Sun in our solar system and is kn own for its reddish appearance. Mars is a rocky planet with a thin atmosphere, and its surface is characterized by volcanoes, canyons, and impact crater.\\nOne of the most intriguing aspects of Mars is its potential for supporting life. While there is currently no evidence of life on Mars, the planet's atmosphere and geology suggest that it may have been habitable in the past. NASA's Curiosity rover has been exploring Mars since 2012, and has discovered evidence of water on the planet, which is a key ingredient for life.\\nMars is also a popular target for space missions and future human settlements. Several space agencies and private companies are planning missions to Mars in the coming years, with the goal of establishing a human presence on the pl anet. The challenges of establishing a human settlement on Mars are significa nt, including the harsh environment, lack of resources, and distance from Earth. However, many experts believe that Mars is the next logical step in the expl\",\n",
    "                \"taylor swift's best friend\": \"Taylor Swift's best friend is a person who has been by her side through thick and thin. Here are some possible candidates:\\n1. Abigail Anderson - Abigail is Taylor's childhood friend and has been a constant presence in her life. The two have been inseparable since they met in kindergarten and have shared countless memories together.\\n2. Selena Gomez - Selena and Taylor have been friends for over a decade and have been through a lot together. They have collaborated on music projects, gone on vacation together, and have been there for each other through personal struggles.\\n3. Liz Rose - Liz is a songwriter and producer who has worked with Taylor on many of her hit songs. The two have a close creative partnership and have been friends for many years.\\n4. Joe Jonas - Joe and Taylor were friends for several years before they started dating in 2008. They have been on-again, off-again since then and have been there for each other during some of the most significant moments in each other's lives.\\n5. Ed Sheeran - Ed and Taylor have been friends\",\n",
    "                \"Agent: I'm here to help you with your Amazon deliver order.\\nCustomer: I didn't get my item\\nAgent: I'm sorry to hear that. Which item was it?\\nCustomer: the blanket\\nAgent:\": \"I see. Can you please provide me with your order number so I can look into this for you?\\nCustomer: I don't have the order number.\\nAgent: Okay, no worries. Can you please tell me the name of the item you didn't receive?\\nCustomer: Blanket\\nAgent: And what was the shipping address for this item?\\nCustomer: 123 Main St\\nAgent: Okay, thank you. I'm going to check on the status of your order. Can you please hold for just a moment?\\nCustomer: (hangs up)\\nThis is a common scenario in customer service, where the agent is trying to gather information from the customer to help resolve their issue. The agent is using active listening skills to gather information from the customer, such as the order number, the name of the item, and the shipping address. The agent is also using empathy to acknowledge the customer's concern and let them know that they are there to help.\",\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model. Please stick to the notebook example.\")\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        return self.responses.get(\n",
    "            prompt,\n",
    "            \"Sorry, I don't know how to respond to that. Please stick to the notebook example.\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b003e",
   "metadata": {},
   "source": [
    "### Non-finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417566f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_finetuned = BasicModelRunner(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_finetuned_output = non_finetuned(\"Tell me how to train my dog to sit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(non_finetuned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(non_finetuned(\"What do you think of Mars?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683515bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(non_finetuned(\"taylor swift's best friend\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509685eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(non_finetuned(\n",
    "    \"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
    "    Customer: I didn't get my item\n",
    "    Agent: I'm sorry to hear that. Which item was it?\n",
    "    Customer: the blanket\n",
    "    Agent:\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a761a",
   "metadata": {},
   "source": [
    "### Finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = BasicModelRunner(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_output = finetuned_model(\"Tell me how to train my dog to sit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d46e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finetuned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f853681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finetuned_model(\"[INST]Tell me how to train my dog to sit[/INST]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ccd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(non_finetuned(\"[INST]Tell me how to train my dog to sit[/INST]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390de9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finetuned_model(\"What do you think of Mars?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1617533",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finetuned_model(\"taylor swift's best friend\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef40284",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finetuned_model(\n",
    "    \"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
    "    Customer: I didn't get my item\n",
    "    Agent: I'm sorry to hear that. Which item was it?\n",
    "    Customer: the blanket\n",
    "    Agent:\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bfa143",
   "metadata": {},
   "source": [
    "## Finetuning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50799840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e018e188",
   "metadata": {},
   "source": [
    "### Pretrained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a738b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_dataset = load_dataset(\"EleutherAI/pile\", split=\"train\", streaming=True)\n",
    "\n",
    "pretrained_dataset = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "print(\"Pretrained dataset:\")\n",
    "top_n = itertools.islice(pretrained_dataset, n)\n",
    "for i in top_n:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3972f1",
   "metadata": {},
   "source": [
    "### Finetuning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c40e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"lamini_docs.jsonl\"\n",
    "instruction_dataset_df = pd.read_json(filename, lines=True)\n",
    "instruction_dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e1c5d",
   "metadata": {},
   "source": [
    "### Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = instruction_dataset_df.to_dict()\n",
    "text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f562706",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"question\" in examples and \"answer\" in examples:\n",
    "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "  text = examples[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf1c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_qa = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "{answer}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f89341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = examples[\"question\"][0]\n",
    "answer = examples[\"answer\"][0]\n",
    "\n",
    "text_with_prompt_template = prompt_template_qa.format(question=question, answer=answer)\n",
    "text_with_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_q = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5f5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset_text_only = []\n",
    "finetuning_dataset_question_answer = []\n",
    "for i in range(num_examples):\n",
    "  question = examples[\"question\"][i]\n",
    "  answer = examples[\"answer\"][i]\n",
    "\n",
    "  text_with_prompt_template_qa = prompt_template_qa.format(question=question, answer=answer)\n",
    "  finetuning_dataset_text_only.append({\"text\": text_with_prompt_template_qa})\n",
    "\n",
    "  text_with_prompt_template_q = prompt_template_q.format(question=question)\n",
    "  finetuning_dataset_question_answer.append({\"question\": text_with_prompt_template_q, \"answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b3c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(finetuning_dataset_text_only[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2cf6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(finetuning_dataset_question_answer[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0cc29d",
   "metadata": {},
   "source": [
    "### Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(f\"../data/lamini_docs_processed.jsonl\", \"w\") as writer:\n",
    "    writer.write_all(finetuning_dataset_question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f54de",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_dataset_name = \"../data/lamini_docs\"\n",
    "finetuning_dataset = load_dataset(finetuning_dataset_name)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8af02a",
   "metadata": {},
   "source": [
    "## Instruction-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a37541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import jsonlines\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModelRunner:\n",
    "    def __init__(self, model_name):\n",
    "        if model_name == \"meta-llama/Llama-2-7b-hf\":\n",
    "            self.responses = {\n",
    "                \"Tell me how to train my dog to sit\": \"Tell me how to train my dog to stay.\\nTell me how to teach my dog to come.\\nTell me how to get my dog to heel.\\nTell me how to stop my dog from jumping.\\nTell me how to make my dog stop barking.\\nTell me how I can get my dog to stop chewing.\\nTell me how to house train my dog.\\nTell me how to potty train my dog.\\nTell me how to train my dog to walk on a leash.\\nTell me how to crate train my dog.\\nTell Me How To Train My Dog To Sit, Stay, Come, Heel, Stop Jumping, Stop Barking, Stop Chewing, House Train, Potty Train, Walk On A Leash, Crate Train, And More!\\nTell Me How To Train Your Dog To Sit, Stay, Heel, Come, Stop Jumping, Stop Chewing, House Trained, Potty Trained, Walk On A Leash, And More!\\nTell me how to train your dog to sit.\\nTell Me How To House Train My Dog.\",\n",
    "            }\n",
    "        elif model_name == \"meta-llama/Llama-2-7b-chat-hf\":\n",
    "            self.responses = {\n",
    "                \"Tell me how to train my dog to sit\": \"on command. Training your dog to sit is a basic obedience command that can be achieved with patience, consistency, and positive reinforcement. Here's a step-by-step guide on how to train your dog to sit:\\n\\n1. Choose a quiet and distraction-free area: Find a quiet room or area where your dog can focus on you without getting distracted.\\n2. Have treats ready: Choose your dog's favorite treats and have them readily available to reward good behavior.\\n3. Stand in front of your dog: Stand in front of your dog and hold a treat close to their nose.\\n4. Move the treat up and back: Slowly move the treat upwards and backwards, towards your dog's tail, while saying \\\"sit\\\" in a calm and clear voice.\\n5. Dog will follow the treat: As you move the treat, your dog should naturally sit down to follow it. The moment they touch their bottom to the ground, give them the treat and praise them.\",\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model. Please stick to the notebook example.\")\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        return self.responses.get(\n",
    "            prompt,\n",
    "            \"Sorry, I don't know how to respond to that. Please stick to the notebook example.\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08123118",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_tuned_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f928a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 5\n",
    "print(\"Instruction-tuned dataset:\")\n",
    "top_m = list(itertools.islice(instruction_tuned_dataset, m))\n",
    "for j in top_m:\n",
    "  print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c9254",
   "metadata": {},
   "source": [
    "### Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d057e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_with_input = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "prompt_template_without_input = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64929146",
   "metadata": {},
   "source": [
    "### Hydrate prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "for j in top_m:\n",
    "  if not j[\"input\"]:\n",
    "    processed_prompt = prompt_template_without_input.format(instruction=j[\"instruction\"])\n",
    "  else:\n",
    "    processed_prompt = prompt_template_with_input.format(instruction=j[\"instruction\"], input=j[\"input\"])\n",
    "\n",
    "  processed_data.append({\"input\": processed_prompt, \"output\": j[\"output\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48971a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(processed_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568fa4c",
   "metadata": {},
   "source": [
    "### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1658543",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(f'alpaca_processed.jsonl', 'w') as writer:\n",
    "    writer.write_all(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a47c5f",
   "metadata": {},
   "source": [
    "### Compare non-instruction-tuned and instruction-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600dbee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_hf = \"lamini/alpaca\"\n",
    "dataset_hf = load_dataset(dataset_path_hf)\n",
    "print(dataset_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_instruct_model = BasicModelRunner(\"meta-llama/Llama-2-7b-hf\")\n",
    "non_instruct_output = non_instruct_model(\"Tell me how to train my dog to sit\")\n",
    "print(\"Not instruction-tuned output (Llama 2 Base):\", non_instruct_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = BasicModelRunner(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "instruct_output = instruct_model(\"Tell me how to train my dog to sit\")\n",
    "print(\"Instruction-tuned output (Llama 2): \", instruct_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744db779",
   "metadata": {},
   "source": [
    "### Check smaller model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1ff2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "    # Tokenize\n",
    "    input_ids = tokenizer.encode(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_tokens\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    device = model.device\n",
    "    generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "    # Strip the prompt\n",
    "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "    return generated_text_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82813bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = load_dataset(finetuning_dataset_path)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = finetuning_dataset[\"test\"][0]\n",
    "print(test_sample)\n",
    "\n",
    "print(inference(test_sample[\"question\"], model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a1ec71",
   "metadata": {},
   "source": [
    "### Compare finedtuned small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab306829",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ea08e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inference(test_sample[\"question\"], instruction_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b74341",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ea4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d06ab",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb33957",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(text)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda483ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded tokens back into text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a483e",
   "metadata": {},
   "source": [
    "### Tokenize multiple texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9846acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_texts = [\"Hi, how are you?\", \"I'm good\", \"Yes\"]\n",
    "encoded_texts = tokenizer(list_texts)\n",
    "print(\"Encoded several texts: \", encoded_texts[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb54712",
   "metadata": {},
   "source": [
    "### Padding and truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "encoded_texts_longest = tokenizer(list_texts, padding=True)\n",
    "print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f3e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.truncation_side = \"left\"\n",
    "encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Using left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
    "print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d5b24",
   "metadata": {},
   "source": [
    "### Prepare instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = \"lamini_docs.jsonl\"\n",
    "instruction_dataset_df = pd.read_json(filename, lines=True)\n",
    "examples = instruction_dataset_df.to_dict()\n",
    "\n",
    "if \"question\" in examples and \"answer\" in examples:\n",
    "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "  text = examples[\"text\"][0]\n",
    "\n",
    "prompt_template = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset = []\n",
    "for i in range(num_examples):\n",
    "    question = examples[\"question\"][i]\n",
    "    answer = examples[\"answer\"][i]\n",
    "    text_with_prompt_template = prompt_template.format(question=question)\n",
    "    finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"One datapoint in the finetuning dataset:\")\n",
    "pprint(finetuning_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375209d1",
   "metadata": {},
   "source": [
    "### Tokenize single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da95065",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n",
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    padding=True\n",
    ")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb16528",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2048\n",
    "max_length = min(\n",
    "    tokenized_inputs[\"input_ids\"].shape[1],\n",
    "    max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ea44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    truncation=True,\n",
    "    max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8f42be",
   "metadata": {},
   "source": [
    "### Tokenize instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    if \"question\" in examples and \"answer\" in examples:\n",
    "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "    elif \"input\" in examples and \"output\" in examples:\n",
    "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "    else:\n",
    "      text = examples[\"text\"][0]\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1edd010",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=filename, split=\"train\")\n",
    "\n",
    "tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4666ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599da4bb",
   "metadata": {},
   "source": [
    "### Prepare test / train splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ddfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510decc9",
   "metadata": {},
   "source": [
    "### Different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = datasets.load_dataset(finetuning_dataset_path)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61475ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "taylor_swift_dataset = \"lamini/taylor_swift\"\n",
    "bts_dataset = \"lamini/bts\"\n",
    "open_llms = \"lamini/open_llms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7feaf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_swiftie = datasets.load_dataset(taylor_swift_dataset)\n",
    "print(dataset_swiftie[\"train\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661ab8b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939a4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lamini\n",
    "\n",
    "lamini.api_url = os.getenv(\"POWERML_PRODUCTION_URL\")\n",
    "lamini.api_key = os.getenv(\"POWERML_PRODUCTION_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1708db8c",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98054145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import transformers\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "global_config = None\n",
    "\n",
    "def initialize_config_and_logging(existing_config=None):\n",
    "    global global_config\n",
    "    global_config = build_config(existing_config)\n",
    "    setup_logging(global_config)\n",
    "    logger.debug(\"Config: \" + str(yaml.dump(global_config.as_dict())))\n",
    "    return global_config\n",
    "\n",
    "def get_config():\n",
    "    global global_config\n",
    "    assert global_config is not None\n",
    "    return global_config\n",
    "\n",
    "def build_config(existing_config=None):\n",
    "    configs = [\n",
    "        # Using config library\n",
    "        config.config_from_env(prefix=\"LLAMA\", separator=\"_\", lowercase_keys=True),\n",
    "    ]\n",
    "\n",
    "    if existing_config:\n",
    "        if isinstance(existing_config, dict):\n",
    "            configs.append(config.config_from_dict(existing_config))\n",
    "        else:\n",
    "            configs.append(existing_config)\n",
    "\n",
    "    config_paths = get_config_paths()\n",
    "\n",
    "    for path in reversed(config_paths):\n",
    "        print(\"Loading builtin config from \" + path)\n",
    "        configs.append(config.config_from_yaml(path, read_from_file=True))\n",
    "\n",
    "    return config.ConfigurationSet(*configs)\n",
    "\n",
    "def get_config_paths():\n",
    "    paths = []\n",
    "\n",
    "def get_config_paths():\n",
    "    paths = []\n",
    "\n",
    "    config_name = \"llama_config\"\n",
    "    config_base = \"configs\"\n",
    "\n",
    "    base_config_path = os.path.join(config_base, config_name + \".yaml\")\n",
    "    if os.path.exists(base_config_path):\n",
    "        paths.append(base_config_path)\n",
    "\n",
    "    local_config_path = os.path.join(config_base, config_name + \"_local.yaml\")\n",
    "    if os.path.exists(local_config_path):\n",
    "        paths.append(local_config_path)\n",
    "\n",
    "    home = os.path.expanduser(\"~\")\n",
    "    home_config_path = os.path.join(home, \".\" + config_name + \".yaml\")\n",
    "    if os.path.exists(home_config_path):\n",
    "        paths.append(home_config_path)\n",
    "\n",
    "    return paths\n",
    "\n",
    "def setup_logging(arguments):\n",
    "    logging_format = \"%(asctime)s - %(levelname)s - %(name)s - %(message)s\"\n",
    "\n",
    "    if arguments[\"verbose\"]:\n",
    "        logging.basicConfig(level=logging.DEBUG, format=logging_format)\n",
    "    elif arguments[\"verbose_info\"]:\n",
    "        logging.basicConfig(level=logging.INFO, format=logging_format)\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.WARNING, format=logging_format)\n",
    "\n",
    "    root_logger = logging.getLogger()\n",
    "\n",
    "    if arguments[\"verbose\"]:\n",
    "        root_logger.setLevel(logging.DEBUG)\n",
    "    elif arguments[\"verbose_info\"]:\n",
    "        root_logger.setLevel(logging.INFO)\n",
    "    else:\n",
    "        root_logger.setLevel(logging.WARNING)\n",
    "\n",
    "    logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"filelock\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"smart_open\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"botocore\").setLevel(logging.WARNING)\n",
    "\n",
    "# Wrapper for data load, split, tokenize for training\n",
    "def tokenize_and_split_data(training_config, tokenizer):\n",
    "  initialized_config = initialize_config_and_logging(training_config)\n",
    "  dataset_path = initialized_config[\"datasets\"][\"path\"]\n",
    "  use_hf = initialized_config[\"datasets\"][\"use_hf\"]\n",
    "  print(\"tokenize\", use_hf, dataset_path)\n",
    "  if use_hf:\n",
    "    dataset = datasets.load_dataset(dataset_path)\n",
    "  else:\n",
    "    dataset = load_dataset(dataset_path, tokenizer)\n",
    "  train_dataset = dataset[\"train\"]\n",
    "  test_dataset = dataset[\"test\"]\n",
    "  return train_dataset, test_dataset\n",
    "\n",
    "# Tokenize and split data\n",
    "def load_dataset(dataset_path, tokenizer):\n",
    "    random.seed(42)\n",
    "    finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    max_length = training_config[\"model\"][\"max_length\"]\n",
    "    tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "        get_tokenize_function(tokenizer, max_length), # returns tokenize_function\n",
    "        batched=True,\n",
    "        batch_size=1,\n",
    "        drop_last_batch=True\n",
    "    )\n",
    "    tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n",
    "    split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "    return split_dataset\n",
    "\n",
    "# Get function for tokenization, based on config parameters\n",
    "def get_tokenize_function(tokenizer, _max_length):\n",
    "\n",
    "  def tokenize_function(examples):\n",
    "    max_length = _max_length\n",
    "\n",
    "    # Set pad token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    if \"question\" in examples and \"answer\" in examples:\n",
    "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "    elif \"input\" in examples and \"output\" in examples:\n",
    "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "    else:\n",
    "      text = examples[\"text\"][0]\n",
    "\n",
    "    # Run tokenizer on all the text (the input and the output)\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "\n",
    "        # Return tensors in a numpy array (other options are pytorch or tf objects)\n",
    "        return_tensors=\"np\",\n",
    "\n",
    "        # Padding type is to pad to the longest sequence in the batch (other option is to a certain max length, or no padding)\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    # Calculate max length\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        max_length\n",
    "    )\n",
    "\n",
    "    if tokenized_inputs[\"input_ids\"].shape[1] > max_length:\n",
    "        logger.warn(\n",
    "            f\"Truncating input from {tokenized_inputs['input_ids'].shape[1]} to {max_length}\"\n",
    "        )\n",
    "\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"]\n",
    "\n",
    "    return tokenized_inputs\n",
    "  return tokenize_function\n",
    "\n",
    "# Load model onto the right device (GPU if available), and load tokenizer\n",
    "def load_model(training_config, load_base_model=False):\n",
    "    model_load_path = \"\"\n",
    "    model_load_path = training_config[\"model\"][\"pretrained_name\"]\n",
    "    logger.debug(f\"Loading default model: {model_load_path}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_load_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_load_path)\n",
    "\n",
    "    logger.debug(\"Copying model to device\")\n",
    "\n",
    "    device_count = torch.cuda.device_count()\n",
    "    if device_count > 0:\n",
    "        logger.debug(\"Select GPU device\")\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        logger.debug(\"Select CPU device\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    logger.debug(\"Copying finished...\")\n",
    "    if \"model_name\" not in training_config:\n",
    "        model_name = model_load_path\n",
    "    else:\n",
    "        model_name = training_config[\"model_name\"]\n",
    "\n",
    "    return model, tokenizer, device, model_name\n",
    "\n",
    "# Trainer class to include logging and history\n",
    "class Trainer(transformers.Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        model_flops,\n",
    "        total_steps,\n",
    "        args=None,\n",
    "        data_collator=None,\n",
    "        train_dataset=None,\n",
    "        eval_dataset=None,\n",
    "        tokenizer=None,\n",
    "        model_init=None,\n",
    "        compute_metrics=None,\n",
    "        callbacks=None,\n",
    "        optimizers=(None, None),\n",
    "    ):\n",
    "        super(Trainer, self).__init__(\n",
    "            model,\n",
    "            args,\n",
    "            data_collator,\n",
    "            train_dataset,\n",
    "            eval_dataset,\n",
    "            tokenizer,\n",
    "            model_init,\n",
    "            compute_metrics,\n",
    "            callbacks,\n",
    "            optimizers,\n",
    "        )\n",
    "\n",
    "        self.total_steps = total_steps\n",
    "        self.model_flops = model_flops\n",
    "        self.start_step = 0\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        if inputs[\"input_ids\"].numel() == 0:\n",
    "\n",
    "          print(\"Inputs: \", inputs)\n",
    "          print(\"Inputs - input_ids\", inputs[\"input_ids\"])\n",
    "          print(\"numel\", inputs[\"input_ids\"].numel())\n",
    "\n",
    "          return torch.tensor(0)\n",
    "        else:\n",
    "          model.train()\n",
    "          inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "          with self.compute_loss_context_manager():\n",
    "              loss = self.compute_loss(model, inputs)\n",
    "\n",
    "          if self.args.n_gpu > 1:\n",
    "              loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "          if self.do_grad_scaling:\n",
    "              self.scaler.scale(loss).backward()\n",
    "          else:\n",
    "              self.accelerator.backward(loss)\n",
    "\n",
    "          return loss.detach() / self.args.gradient_accumulation_steps\n",
    "\n",
    "    def log(self, logs):\n",
    "        \"\"\"\n",
    "        Log `logs` on the various objects watching training.\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "        Args:\n",
    "            logs (`Dict[str, float]`):\n",
    "                The values to log.\n",
    "        \"\"\"\n",
    "        if self.state.epoch is not None:\n",
    "            logs[\"epoch\"] = round(self.state.epoch, 2)\n",
    "\n",
    "        self.update_log_timing(logs)\n",
    "\n",
    "        output = {**logs, **{\"step\": self.state.global_step}}\n",
    "        self.update_history(output)\n",
    "\n",
    "        logger.debug(\"Step (\" + str(self.state.global_step) + \") Logs: \" + str(logs))\n",
    "        self.control = self.callback_handler.on_log(\n",
    "            self.args, self.state, self.control, logs\n",
    "        )\n",
    "\n",
    "    def update_log_timing(self, logs):\n",
    "        if len(self.state.log_history) == 0:\n",
    "            self.start_time = time.time()\n",
    "            logs[\"iter_time\"] = 0.0\n",
    "            logs[\"flops\"] = 0.0\n",
    "            logs[\"remaining_time\"] = 0.0\n",
    "            self.start_step = self.state.global_step\n",
    "        elif self.state.global_step > self.start_step:\n",
    "            logs[\"iter_time\"] = (time.time() - self.start_time) / (\n",
    "                self.state.global_step - self.start_step\n",
    "            )\n",
    "            logs[\"flops\"] = self.model_flops / logs[\"iter_time\"]\n",
    "            logs[\"remaining_time\"] = (self.total_steps - self.state.global_step) * logs[\n",
    "                \"iter_time\"\n",
    "            ]\n",
    "\n",
    "    def update_history(self, output):\n",
    "        if \"eval_loss\" in output:\n",
    "            return\n",
    "        if len(self.state.log_history) > 0:\n",
    "            smoothing_window = 100\n",
    "            p = 1.0 / smoothing_window\n",
    "            if \"loss\" in output:\n",
    "                output[\"loss\"] = output[\"loss\"] * p + self.state.log_history[-1][\n",
    "                    \"loss\"\n",
    "                ] * (1.0 - p)\n",
    "        self.state.log_history.append(output)\n",
    "\n",
    "\n",
    "def sample_history(history):\n",
    "    if not history:\n",
    "        return history\n",
    "    step = (len(history) + 99) // 100\n",
    "\n",
    "    return history[0 : len(history) : step]\n",
    "\n",
    "# Copy file\n",
    "def smart_copy(remote_path, local_path):\n",
    "    with open(remote_path, \"wb\") as remote_file:\n",
    "        with open(local_path, \"rb\") as local_file:\n",
    "            remote_file.write(local_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce611b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "global_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ead900",
   "metadata": {},
   "source": [
    "### Explore model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3a3321",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModelRunner:\n",
    "    def __init__(self, model_name):\n",
    "        if (\n",
    "            model_name\n",
    "            == \"06ad41e68cd839fb475a0c1a4ee7a3ad398228df01c9396a97788295d5a0f8bb\"\n",
    "        ):\n",
    "            self.responses = {\n",
    "                \"Can Lamini generate technical documentation or user manuals for software projects?\": \"Yes, Lamini can generate technical documentation or user manuals.\"\n",
    "            }\n",
    "        elif model_name == \"EleutherAI/pythia-410m\":\n",
    "            self.responses = {}\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model. Please stick to the notebook example.\")\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        return self.responses.get(\n",
    "            prompt,\n",
    "            \"Sorry, I don't know how to respond to that. Please stick to the notebook example.\",\n",
    "        )\n",
    "\n",
    "    def load_data_from_jsonlines(self, name, input_key, output_key):\n",
    "        if (\n",
    "            name != \"lamini_docs.jsonl\"\n",
    "            and input_key != \"question\"\n",
    "            and output_key != \"answer\"\n",
    "        ):\n",
    "            raise ValueError(\"Invalid args. Please stick to the notebook example.\")\n",
    "\n",
    "    def train(self, is_public):\n",
    "        return \"Training job submitted!\\nFinetuning process completed, model name is: c8ff4b19807dd10007a7f3b51ccc09dd 8237ef3d47410dae13394c072a12978\"\n",
    "\n",
    "    def evaluate(self):\n",
    "        json = {\n",
    "            \"job_id\": 2349,\n",
    "            \"eval_results\": [\n",
    "                {\n",
    "                    \"input\": \"Does Lamini have the ability to understand and generate code for audio processing tasks?\",\n",
    "                    \"outputs\": [\n",
    "                        {\n",
    "                            \"model_name\": \"c8ff4b19807dd10007a73b51ccc09dd8237ef3d47410dae13394fc072a12978\",\n",
    "                            \"output\": \"Yes, Lamini has the ability to understand and generate code.\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"model_name\": \"Base model (EleutherAI/pythia-410m)\",\n",
    "                            \"output\": \"In\\nA: \\n\\nLamini is a very good language for audio processing.\\n\\nA: \\n\\nI think you are looking for a language that can be used to write audio code. \\n\\nA: \\n\\nLanguages like C, C++, Java, Python, C#, C++, C++ and others are good for audio coding. \\n\\nA: \\n\\nYou can use a language like C, C++, Java, C#, C++, C++ or C++ for audio coding. \\n\\nA language that can be used to write code for audio coding is C.\\n\\nA:\\n\\n is a good language for audio coding. \\n\\nA good language for audio coding is C.\\nC++ is a good language for audio coding, but it is not a good language for audio coding. \\n\\n\",\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"input\": \"Is it possible to control the level of detail in the generated output?\",\n",
    "                    \"outputs\": [\n",
    "                        {\n",
    "                            \"model_name\": \"c8ff4b19807dd10007a7f3b51ccc09dd8237ef3d47410dae13394fc072a12978\",\n",
    "                            \"output\": 'Yes, it is possible to control the level of detail provided in the generated output. To do so, you can use the \"level\" parameter in the \"generate_output\" method. This parameter controls the level of detail in the generated text. The default value is \"none\".',\n",
    "                        },\n",
    "                        {\n",
    "                            \"model_name\": \"Base model (EleutherAI/pythia-410m)\",\n",
    "                            \"output\": '\\n\\nA:\\n\\nYou can use the following code to control the level of detail in your output:\\n#include <iostream>\\n#include <string>\\n#include <vector>\\n\\nusing namespace std;\\n\\nint main()\\n{\\n string s;\\n s = \"Hello World\"; \\n cout << s << endl; \\n return 0;\\n}\\n\\nOutput:\\nHello World In\\nA: \\n\\nYou could use the following code to control the output level:\\n#inc',\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"input\": \"What are the common challenges when fine-tuning large language models?\",\n",
    "                    \"outputs\": [\n",
    "                        {\n",
    "                            \"model_name\": \"c8ff4b19807dd10007a73b51ccc09dd8237ef3d47410dae13394fc072a12978\",\n",
    "                            \"output\": \"Common challenges include computational resources, data quality and quantity, catastrophic forgetting, and ensuring unbiased and safe outputs.\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"model_name\": \"Base model (EleutherAI/pythia-410m)\",\n",
    "                            \"output\": \"A: \\n\\nThere are many challenges when fine-tuning large language models. The most common challenges are:\\n\\n1.  **Data scarcity:** Large language models require a large amount of data to be fine-tuned. If you do not have enough data, the model will not be able to learn the desired task.\\n2.  **Computational resources:** Fine-tuning large language models requires a lot of computational resources. You need to have a powerful GPU or CPU to fine-tune the model.\\n3.  **Overfitting:** If you fine-tune the model for too long, it will overfit to the training data. This means that the model will not be able to generalize to new data.\\n4.  **Catastrophic forgetting:** When you fine-tune a model on a new task, it may forget the knowledge it learned from the previous task. This is called catastrophic forgetting.\\n5.  **Bias:** Large language models can inherit biases from the training data. If the training data contains biases, the model will also contain biases.\",\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"input\": \"Can Lamini handle multilingual text generation?\",\n",
    "                    \"outputs\": [\n",
    "                        {\n",
    "                            \"model_name\": \"c8ff4b19807dd10007a73b51ccc09dd8237ef3d47410dae13394fc072a12978\",\n",
    "                            \"output\": \"Yes, Lamini supports multilingual text generation across various languages.\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"model_name\": \"Base model (EleutherAI/pythia-410m)\",\n",
    "                            \"output\": \"In\\nA: \\n\\nYes, Lamini can handle multilingual text generation. \\n\\nA: \\n\\nLamini supports multiple languages, including English, Spanish, French, German, Italian, Portuguese, Dutch, Russian, Chinese, Japanese, Korean, Arabic, Hindi, and more. \\n\\nA: \\n\\nLamini is a very powerful tool that can be used to generate text in multiple languages. \\n\\nA: \\n\\nLamini is a very powerful tool that can be used to generate text in multiple languages, such as English, Spanish, French, German, Italian, Portuguese, Dutch, Russian, Chinese, Japanese, Korean, Arabic, Hindi, and more.\\n\\nA: \\n\\nLamini can be used to generate text in multiple languages, such as English, Spanish, French, German, Italian, Portuguese, Dutch, Russian, Chinese, Japanese, Korean, Arabic, Hindi, and more.\\n\\n\",\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "        return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef262a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"../data/lamini_docs.jsonl\"\n",
    "dataset_path = f\"/content/{dataset_name}\"\n",
    "use_hf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455178ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"lamini/lamini_docs\"\n",
    "use_hf = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85adc4cb",
   "metadata": {},
   "source": [
    "### Set up mode, training config, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39951c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c0e951",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a686bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a208053",
   "metadata": {},
   "source": [
    "### Define inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdaf73d",
   "metadata": {},
   "source": [
    "### Try model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88779f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73eb3c7",
   "metadata": {},
   "source": [
    "### Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9858e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eb2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
    "output_dir = trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b439d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=1,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=max_steps,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_flops = (\n",
    "  base_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, training_config[\"model\"][\"max_length\"])\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c09229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "trainer.do_grad_scaling = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f48838",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0962068",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec8df3",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab55b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f\"{output_dir}/final\"\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c924e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_slightly_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b8a48",
   "metadata": {},
   "source": [
    "### Run slightly trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d420b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_question)\n",
    "\n",
    "print(\"Finetuned slightly model's answer: \")\n",
    "print(inference(test_question, finetuned_slightly_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c9100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answer = test_dataset[0]['answer']\n",
    "print(\"Target answer output (test):\", test_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8acc36f",
   "metadata": {},
   "source": [
    "### Run same model trained for two epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9c226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "\n",
    "finetuned_longer_model.to(device)\n",
    "print(\"Finetuned longer model's answer: \")\n",
    "print(inference(test_question, finetuned_longer_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb22c2a6",
   "metadata": {},
   "source": [
    "### Run much larger trained model and explore moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f84566",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_finetuned_model = BasicModelRunner(model_name_to_id[\"bigger_model_name\"])\n",
    "bigger_finetuned_output = bigger_finetuned_model(test_question)\n",
    "print(\"Bigger (2.8B) finetuned model (test): \", bigger_finetuned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(train_dataset)):\n",
    " if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n",
    "  print(i, train_dataset[i][\"question\"], train_dataset[i][\"answer\"])\n",
    "  count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4edd6",
   "metadata": {},
   "source": [
    "### Explore moderation with small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f9916",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c0dc8",
   "metadata": {},
   "source": [
    "### Explore moderation with finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254f0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inference(\"What do you think of Mars?\", finetuned_longer_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fb688",
   "metadata": {},
   "source": [
    "### Finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModelRunner(\"EleutherAI/pythia-410m\") \n",
    "model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n",
    "model.train(is_public=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dcfae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07821f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lofd = []\n",
    "for e in out['eval_results']:\n",
    "    q  = f\"{e['input']}\"\n",
    "    at = f\"{e['outputs'][0]['output']}\"\n",
    "    ab = f\"{e['outputs'][1]['output']}\"\n",
    "    di = {'question': q, 'trained model': at, 'Base Model' : ab}\n",
    "    lofd.append(di)\n",
    "df = pd.DataFrame.from_dict(lofd)\n",
    "style_df = df.style.set_properties(**{'text-align': 'left'})\n",
    "style_df = style_df.set_properties(**{\"vertical-align\": \"text-top\"})\n",
    "style_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02fe76",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443814bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import difflib\n",
    "import pandas as pd\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "global_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"lamini/lamini_docs\")\n",
    "\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c217a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset[0][\"question\"])\n",
    "print(test_dataset[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff5d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lamini/lamini_docs_finetuned\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46594c34",
   "metadata": {},
   "source": [
    "### Setup evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feddf559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_exact_match(a, b):\n",
    "    return a.strip() == b.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fde618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  input_ids = tokenizer.encode(\n",
    "      text,\n",
    "      return_tensors=\"pt\",\n",
    "      truncation=True,\n",
    "      max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b9d8d5",
   "metadata": {},
   "source": [
    "### Run model and compare to expected answer model and compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc9304",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = test_dataset[0][\"question\"]\n",
    "generated_answer = inference(test_question, model, tokenizer)\n",
    "print(test_question)\n",
    "print(generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f706b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = test_dataset[0][\"answer\"]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match = is_exact_match(generated_answer, answer)\n",
    "print(exact_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b145b5fd",
   "metadata": {},
   "source": [
    "### Run on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "metrics = {'exact_matches': []}\n",
    "predictions = []\n",
    "for i, item in tqdm(enumerate(test_dataset)):\n",
    "    print(\"i Evaluating: \" + str(item))\n",
    "    question = item['question']\n",
    "    answer = item['answer']\n",
    "\n",
    "    try:\n",
    "      predicted_answer = inference(question, model, tokenizer)\n",
    "    except:\n",
    "      continue\n",
    "    predictions.append([predicted_answer, answer])\n",
    "\n",
    "    #fixed: exact_match = is_exact_match(generated_answer, answer)\n",
    "    exact_match = is_exact_match(predicted_answer, answer)\n",
    "    metrics['exact_matches'].append(exact_match)\n",
    "\n",
    "    if i > n and n != -1:\n",
    "      break\n",
    "print('Number of exact matches: ', sum(metrics['exact_matches']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad8c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predictions, columns=[\"predicted_answer\", \"target_answer\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eb9780",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset_path = \"lamini/lamini_docs_evaluation\"\n",
    "evaluation_dataset = datasets.load_dataset(evaluation_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc3e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4e0e0",
   "metadata": {},
   "source": [
    "### Try ARC benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ec1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python lm-evaluation-harness/main.py --model hf-causal --model_args pretrained=lamini/lamini_docs_finetuned --tasks arc_easy --device cpu"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
